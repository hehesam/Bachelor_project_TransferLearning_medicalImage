{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 352/352 [00:00<00:00, 176kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 805/805 [00:00<?, ?B/s] \n",
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\convnext\\feature_extraction_convnext.py:28: FutureWarning: The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Downloading pytorch_model.bin: 100%|██████████| 94.4M/94.4M [01:18<00:00, 1.20MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"Alia-Mohammed/resnet-50-finetuned-brain-tumor\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"Alia-Mohammed/resnet-50-finetuned-brain-tumor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinModel(\n",
       "  (embeddings): SwinEmbeddings(\n",
       "    (patch_embeddings): SwinPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): SwinEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): SwinStage(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinLayer(\n",
       "            (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SwinAttention(\n",
       "              (self): SwinSelfAttention(\n",
       "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): SwinSelfOutput(\n",
       "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SwinDropPath(p=0.1)\n",
       "            (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (intermediate): SwinIntermediate(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): SwinOutput(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): SwinPatchMerging(\n",
       "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinStage(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinLayer(\n",
       "            (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SwinAttention(\n",
       "              (self): SwinSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): SwinSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SwinDropPath(p=0.1)\n",
       "            (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (intermediate): SwinIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): SwinOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): SwinPatchMerging(\n",
       "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): SwinStage(\n",
       "        (blocks): ModuleList(\n",
       "          (0-17): 18 x SwinLayer(\n",
       "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SwinAttention(\n",
       "              (self): SwinSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): SwinSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SwinDropPath(p=0.1)\n",
       "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (intermediate): SwinIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): SwinOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): SwinPatchMerging(\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): SwinStage(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinLayer(\n",
       "            (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SwinAttention(\n",
       "              (self): SwinSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): SwinSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SwinDropPath(p=0.1)\n",
       "            (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (intermediate): SwinIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): SwinOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# Load configuration and model\n",
    "config = AutoConfig.from_pretrained(r'C:\\LMS\\8\\project\\Medical student\\CQ500_data\\models\\brain_tumor_classification')\n",
    "model = AutoModel.from_pretrained(r'C:\\LMS\\8\\project\\Medical student\\CQ500_data\\models\\brain_tumor_classification', config=config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 327/327 [00:00<?, ?B/s] \n",
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\moheb\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 717/717 [00:00<00:00, 717kB/s]\n",
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Downloading pytorch_model.bin: 100%|██████████| 343M/343M [04:21<00:00, 1.31MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"youngp5/Brain-Tumor-Detector\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"youngp5/Brain-Tumor-Detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 189/189 [00:00<?, ?B/s] \n",
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\moheb\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading best.pt: 100%|██████████| 31.7M/31.7M [00:05<00:00, 5.39MB/s]\n",
      "Ultralytics YOLOv8.0.43  Python-3.11.3 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "YOLOv8m-cls summary (fused): 103 layers, 15765218 parameters, 0 gradients, 41.6 GFLOPs\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg to zidane.jpg...\n",
      "100%|██████████| 165k/165k [00:01<00:00, 158kB/s]  \n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to C:\\Users\\moheb\\AppData\\Roaming\\Ultralytics\\Arial.ttf...\n",
      "100%|██████████| 755k/755k [00:02<00:00, 315kB/s]  \n",
      "image 1/1 C:\\LMS\\8\\project\\Medical student\\CQ500_data\\zidane.jpg: 224x224 PNEUMONIA 0.98, NORMAL 0.02, 29.1ms\n",
      "Speed: 2.0ms preprocess, 29.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.01983, 0.98017], device='cuda:0')\n",
      "{'NORMAL': 0.01982736960053444, 'PNEUMONIA': 0.9801726937294006}\n"
     ]
    }
   ],
   "source": [
    "from ultralyticsplus import YOLO, postprocess_classify_output\n",
    "\n",
    "# load model\n",
    "model = YOLO('keremberke/yolov8m-chest-xray-classification')\n",
    "\n",
    "# set model parameters\n",
    "model.overrides['conf'] = 0.25  # model confidence threshold\n",
    "\n",
    "# set image\n",
    "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n",
    "\n",
    "# perform inference\n",
    "results = model.predict(image)\n",
    "\n",
    "# observe results\n",
    "print(results[0].probs) # [0.1, 0.2, 0.3, 0.4]\n",
    "processed_result = postprocess_classify_output(model, result=results[0])\n",
    "print(processed_result) # {\"cat\": 0.4, \"dog\": 0.6}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 228/228 [00:00<00:00, 228kB/s]\n",
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\moheb\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 737/737 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 343M/343M [01:11<00:00, 4.80MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"nickmuchi/vit-finetuned-chest-xray-pneumonia\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"nickmuchi/vit-finetuned-chest-xray-pneumonia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 503/503 [00:00<?, ?B/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 419/419 [00:00<?, ?B/s] \n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 621kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 712k/712k [00:00<00:00, 1.98MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<?, ?B/s] \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 2.91k/2.91k [00:00<?, ?B/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.58G/1.58G [05:22<00:00, 4.89MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 141/141 [00:00<00:00, 70.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"daniyal214/finetuned-git-large-chest-xrays\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"daniyal214/finetuned-git-large-chest-xrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 228/228 [00:00<?, ?B/s] \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 745/745 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 343M/343M [01:11<00:00, 4.78MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"DunnBC22/vit-base-patch16-224-in21k_covid_19_ct_scans\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"DunnBC22/vit-base-patch16-224-in21k_covid_19_ct_scans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 402/402 [00:00<00:00, 402kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 977/977 [00:00<?, ?B/s] \n",
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\yolos\\feature_extraction_yolos.py:28: FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Downloading pytorch_model.bin: 100%|██████████| 26.0M/26.0M [00:13<00:00, 1.91MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoFeatureExtractor, AutoModelForObjectDetection\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"DunnBC22/yolos-tiny-Brain_Tumor_Detection\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"DunnBC22/yolos-tiny-Brain_Tumor_Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"DunnBC22/vit-base-patch16-224-in21k_covid_19_ct_scans\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"DunnBC22/vit-base-patch16-224-in21k_covid_19_ct_scans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/DunnBC22/efficientformer-l3-300-Brain_Tumors_Image_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 122M/122M [01:56<00:00, 1.05MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-classification\", model=\"DunnBC22/efficientformer-l3-300-Brain_Tumors_Image_Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/google/vit-base-patch16-224-in21k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 346M/346M [04:51<00:00, 1.18MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"feature-extraction\", model=\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/microsoft/swin-base-patch4-window7-224-in22k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.67M/1.67M [00:01<00:00, 1.62MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 437M/437M [01:47<00:00, 4.08MB/s] \n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 255/255 [00:00<?, ?B/s] \n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-classification\", model=\"microsoft/swin-base-patch4-window7-224-in22k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/hustvl/yolos-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 275/275 [00:00<00:00, 273kB/s]\n",
      "c:\\Users\\moheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\yolos\\feature_extraction_yolos.py:28: FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.\n",
      "  warnings.warn(\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 4.13k/4.13k [00:00<?, ?B/s]\n",
      "Downloading model.safetensors: 100%|██████████| 26.0M/26.0M [00:20<00:00, 1.25MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoFeatureExtractor, AutoModelForObjectDetection\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
